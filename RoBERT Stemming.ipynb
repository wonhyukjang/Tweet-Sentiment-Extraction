{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import math\n",
    "from copy import deepcopy as dc\n",
    "import gc\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Pre-trained roverta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = '../1.Data/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "EPOCHS = 1 # originally 3\n",
    "BATCH_SIZE = 32 # originally 32\n",
    "PAD_ID = 1\n",
    "SEED = 88888\n",
    "LABEL_SMOOTHING = 0.1\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "sentiment_id = {'positive': 1313, 'neutral': 7974, 'negative': 2430}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../1.Data/train.csv').fillna('')\n",
    "n = train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " token \"_________________________________\" will be used to replace the selected_text in the text during modifications, so that we don't lose track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet, stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop += [\"_________________________________\", \"u\"]\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "punct.remove(\"-\")\n",
    "punct.append(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word of a sentence that isn't a stopword, we will randomly choose between itself and all his synonyms, in order to replace it in the modified sentence. The first function is for getting synonyms of a word, and probabilities of selecting each one of them when we will randomly rebuild a modified sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Get synonyms of a word\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    \n",
    "    if word.lower() in stop:\n",
    "        return [word], [1]\n",
    "    \n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            # Remove Hypoon\n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            \n",
    "            # Only remain characters\n",
    "            if synonym not in stop_words:\n",
    "                synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "                try:\n",
    "                    if nltk.pos_tag(synonym)[0][0] in ['VB','VBD','VBG','VBN','VBP','VBZ','RB','RBR','RBS','RP','JJ','JJR','JJS','CC']:\n",
    "                        synonyms.add(synonym) \n",
    "                except:\n",
    "                    None\n",
    "    if word not in synonyms:\n",
    "        synonyms.add(word)\n",
    "        \n",
    "    n = len(synonyms)\n",
    "    \n",
    "    if n == 1: # we didn't find any synonyms for that word, therefore we will try to check if it's not because of some punctuation interfering\n",
    "        word_ = \"\".join(list(filter(lambda x: x not in punct, word)))\n",
    "        if word_.lower() in stop:\n",
    "            return [word, word_], [0.5, 0.5]\n",
    "        for syn in wordnet.synsets(word_): \n",
    "            for l in syn.lemmas(): \n",
    "                synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "                synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "                synonyms.add(synonym) \n",
    "        if word_ not in synonyms:\n",
    "            synonyms.add(word_)\n",
    "            \n",
    "    n = len(synonyms)\n",
    "    if n == 1:\n",
    "        probabilities = [1]\n",
    "    else:\n",
    "        probabilities = [0.5 if w==word else 0.5/(n-1) for w in synonyms]\n",
    "    \n",
    "    return list(synonyms), probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of synonym generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For word sad, synonyms and corresponding probabilities are :\n",
      "(['sad', 'lamentable', 'distressing', 'pitiful', 'deplorable', 'sorry'], [0.5, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
      "--------------------\n",
      "For word SAD, synonyms and corresponding probabilities are :\n",
      "(['sad', 'lamentable', 'distressing', 'SAD', 'pitiful', 'deplorable', 'sorry'], [0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.5, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333])\n",
      "--------------------\n",
      "For word Sad..., synonyms and corresponding probabilities are :\n",
      "(['sad', 'lamentable', 'Sad', 'distressing', 'pitiful', 'deplorable', 'sorry', 'Sad...'], [0.07142857142857142, 0.07142857142857142, 0.07142857142857142, 0.07142857142857142, 0.07142857142857142, 0.07142857142857142, 0.07142857142857142, 0.5])\n",
      "--------------------\n",
      "For word saaaaad, synonyms and corresponding probabilities are :\n",
      "(['saaaaad'], [1])\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for word in ['sad', 'SAD', 'Sad...', 'saaaaad']:\n",
    "    print(f'For word {word}, synonyms and corresponding probabilities are :')\n",
    "    print(get_synonyms(word))\n",
    "    print('-'*20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_words(words):\n",
    "    words = words.split()\n",
    "    if len(words) < 2:\n",
    "        return \" \".join(words), False\n",
    "    random_idx = np.random.randint(0, len(words)-1)\n",
    "    words[random_idx], words[random_idx+1] = words[random_idx+1], words[random_idx] \n",
    "    return \" \".join(words), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun is shining today, this me makes feel so good !\n",
      "The sun is shining today, makes this me feel so good !\n",
      "sun The is shining today, this makes me feel so good !\n",
      "The is sun shining today, this makes me feel so good !\n",
      "The sun is shining today, this makes me feel so ! good\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(swap_words('The sun is shining today, this makes me feel so good !')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If our selected word token has adjective, adverb, verb, change them to their synonym to make deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_row(row, n_samples=1): \n",
    "    text, selected_text, textID = row['text'], row['selected_text'], row['textID']\n",
    "    \n",
    "    oth_text = text.replace(selected_text, \" _________________________________ \")\n",
    "    \n",
    "    new_selected_text = [get_synonyms(word) for word in selected_text.split()]\n",
    "    \n",
    "    new_oth_text = [get_synonyms(word) for word in oth_text.split()]\n",
    "    \n",
    "    new_sentences = [row]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        selected_text_ = \" \".join([np.random.choice(l_syn, p=p, replace=True) for l_syn, p in new_selected_text])\n",
    "        text_ = oth_text.replace(\"_________________________________\", selected_text_)\n",
    "        if not selected_text_ in text_:\n",
    "            print(f'Original : {text} with target {selected_text}, oth_text {oth_text}\\nTransformed : {text_} with target {selected_text_}, oth_text {oth_text_}')\n",
    "            continue\n",
    "        row2 = dc(row)\n",
    "        row2['text'] = text_\n",
    "        row2['selected_text'] = selected_text_\n",
    "        row2['textID'] = f'new_{textID}'\n",
    "        new_sentences.append(row2)\n",
    "        \n",
    "    new_rows = pd.concat(new_sentences, axis=1).transpose().drop_duplicates(subset=['text'], inplace=False, ignore_index=True)\n",
    "    new_rows = new_rows.loc[new_rows['text'].apply(len)<150]\n",
    "    counter = 0\n",
    "    \n",
    "    for i, row in new_rows.iterrows():\n",
    "        if row['textID'][:4] == 'new_':\n",
    "            row['textID'] = row['textID']+f'_{counter}'\n",
    "            counter += 1\n",
    "    return new_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of new sentence with stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4ab4d74f14</td>\n",
       "      <td>so go back for more</td>\n",
       "      <td>so go back for more</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_4ab4d74f14_0</td>\n",
       "      <td>so offer back for more</td>\n",
       "      <td>so offer back for more</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_4ab4d74f14_1</td>\n",
       "      <td>so go plump for for more</td>\n",
       "      <td>so go plump for for more</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new_4ab4d74f14_2</td>\n",
       "      <td>so go back for more</td>\n",
       "      <td>so go back for more</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_4ab4d74f14_3</td>\n",
       "      <td>so ecstasy back for more</td>\n",
       "      <td>so ecstasy back for more</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>new_4ab4d74f14_4</td>\n",
       "      <td>so go hind for more</td>\n",
       "      <td>so go hind for more</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>new_4ab4d74f14_5</td>\n",
       "      <td>so go support for more</td>\n",
       "      <td>so go support for more</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             textID                         text             selected_text  \\\n",
       "0        4ab4d74f14          so go back for more       so go back for more   \n",
       "1  new_4ab4d74f14_0      so offer back for more     so offer back for more   \n",
       "2  new_4ab4d74f14_1    so go plump for for more   so go plump for for more   \n",
       "3  new_4ab4d74f14_2         so go back for more        so go back for more   \n",
       "4  new_4ab4d74f14_3    so ecstasy back for more   so ecstasy back for more   \n",
       "5  new_4ab4d74f14_4         so go hind for more        so go hind for more   \n",
       "6  new_4ab4d74f14_5      so go support for more     so go support for more   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1   neutral  \n",
       "2   neutral  \n",
       "3   neutral  \n",
       "4   neutral  \n",
       "5   neutral  \n",
       "6   neutral  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_row(train.loc[np.random.choice(train.shape[0])], n_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpTrain = train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5  28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6  6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7  50e14c0bb8                                         Soooo high   \n",
       "8  e050245fbd                                        Both of you   \n",
       "9  fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                I`d have responded, if I were going   neutral  \n",
       "1                                           Sooo SAD  negative  \n",
       "2                                        bullying me  negative  \n",
       "3                                     leave me alone  negative  \n",
       "4                                      Sons of ****,  negative  \n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                fun  positive  \n",
       "7                                         Soooo high   neutral  \n",
       "8                                        Both of you   neutral  \n",
       "9                       Wow... u just became cooler.  positive  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = [new_row(row, n_samples=2) for _, row in train.iterrows()]\n",
    "\n",
    "augmented_data = pd.concat(temp, axis=0)#.sample(frac=1)\n",
    "\n",
    "del temp\n",
    "gc.collect()\n",
    "augmented_data.drop_duplicates(subset=['text'], inplace=False, ignore_index=True)\n",
    "augmented_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data.to_csv('extended_train.csv', index=False)\n",
    "train = augmented_data\n",
    "del augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_len'] = train['text'].apply(lambda x: len(x))\n",
    "train = train.loc[train.text_len<150]\n",
    "train.drop(columns=[\"text_len\"], inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('extended_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('extended_train.csv').fillna('')\n",
    "n = train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess RoBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask[k,:len(enc.ids)+3] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+2] = 1\n",
    "        end_tokens[k,toks[-1]+2] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../1.Data/test.csv').fillna('')\n",
    "\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+3] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_weights(model, dst_fn):\n",
    "    weights = model.get_weights()\n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "\n",
    "def load_weights(model, weight_fn):\n",
    "    with open(weight_fn, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    return model\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    # this is required as `model.predict` needs a fixed size!\n",
    "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    \n",
    "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
    "    return model, padded_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "1941/1941 [==============================] - 11189s 6s/step - loss: 2.5600 - activation_loss: 1.2899 - activation_1_loss: 1.2701 - val_loss: 2.2242 - val_activation_loss: 1.1315 - val_activation_1_loss: 1.0927\n",
      "Loading model...\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 219s 2s/step\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "1941/1941 [==============================] - 10320s 5s/step - loss: 2.6029 - activation_loss: 1.3132 - activation_1_loss: 1.2897 - val_loss: 2.2819 - val_activation_loss: 1.1603 - val_activation_1_loss: 1.1216\n",
      "Loading model...\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 176s 2s/step\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "1941/1941 [==============================] - 8100s 4s/step - loss: 2.6163 - activation_loss: 1.3202 - activation_1_loss: 1.2961 - val_loss: 2.2503 - val_activation_loss: 1.1456 - val_activation_1_loss: 1.1047\n",
      "Loading model...\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 187s 2s/step\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "1941/1941 [==============================] - 8605s 4s/step - loss: 2.6015 - activation_loss: 1.3106 - activation_1_loss: 1.2910 - val_loss: 2.2488 - val_activation_loss: 1.1383 - val_activation_1_loss: 1.1105\n",
      "Loading model...\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 185s 2s/step\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "1941/1941 [==============================] - 7645s 4s/step - loss: 2.5593 - activation_loss: 1.2889 - activation_1_loss: 1.2704 - val_loss: 2.2234 - val_activation_loss: 1.1279 - val_activation_1_loss: 1.0955\n",
      "Loading model...\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 181s 2s/step\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED)\n",
    "\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model, padded_model = build_model()\n",
    "        \n",
    "    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
    "    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n",
    "    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
    "    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n",
    "    # sort the validation data\n",
    "    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n",
    "    inpV = [arr[shuffleV] for arr in inpV]\n",
    "    targetV = [arr[shuffleV] for arr in targetV]\n",
    "    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # sort and shuffle: We add random numbers to not have the same order in each epoch\n",
    "        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n",
    "        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n",
    "        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n",
    "        batch_inds = np.random.permutation(num_batches)\n",
    "        shuffleT_ = []\n",
    "        for batch_ind in batch_inds:\n",
    "            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n",
    "        shuffleT = np.concatenate(shuffleT_)\n",
    "        # reorder the input data\n",
    "        inpT = [arr[shuffleT] for arr in inpT]\n",
    "        targetT = [arr[shuffleT] for arr in targetT]\n",
    "        model.fit(inpT, targetT, \n",
    "            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n",
    "            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n",
    "        save_weights(model, weight_fn)\n",
    "\n",
    "    print('Loading model...')\n",
    "    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    load_weights(model, weight_fn)\n",
    "\n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()\n",
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>cde0016d6d</td>\n",
       "      <td>http://twitpic.com/4wp8s - My ear hurts, and THIS is my ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>hurts,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>acf7eddf2b</td>\n",
       "      <td>****. You could have just called or told me in person. ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>****.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>7315faa6ce</td>\n",
       "      <td>oh mannn i`m gonna be there tomorroww</td>\n",
       "      <td>neutral</td>\n",
       "      <td>oh mannn i`m gonna be there tomorroww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>5a8f396cb2</td>\n",
       "      <td>so who is in for bring at HK lounge tomorrow ? $12 all y...</td>\n",
       "      <td>positive</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>fc6128f4e5</td>\n",
       "      <td>ok i just spent like ï¿½50 on soundtracks, a galaxy clas...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ok i just spent like ï¿½50 on soundtracks, a galaxy cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>c4393e4cba</td>\n",
       "      <td>HAPPY MOTHERS DAY!  Tell ur mom that`s she an awesome m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6612e42b28</td>\n",
       "      <td>Kyle is Cody`s wee bro!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>kyle is cody`s wee bro!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>4e05652b8c</td>\n",
       "      <td>Time to play the drums</td>\n",
       "      <td>neutral</td>\n",
       "      <td>time to play the drums</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>63491134ab</td>\n",
       "      <td>i made a vid for you proving my skiLLs that you denied ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i made a vid for you proving my skiLLs that you denied ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>ba7df25a0e</td>\n",
       "      <td>Chrystina Grace Timberlake has a ring to it!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>chrystina grace timberlake has a ring to it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>822cf03d0d</td>\n",
       "      <td>Internet 1 drawing 0 ....**** I`m backing up blackberry...</td>\n",
       "      <td>negative</td>\n",
       "      <td>internet 1 drawing 0 ....**** i`m backing up blackberry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>d65a8658b0</td>\n",
       "      <td>I just got through doing my work now I can take the wh...</td>\n",
       "      <td>positive</td>\n",
       "      <td>its great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>53969f2468</td>\n",
       "      <td>Hey! That`s my school! I hate that place.</td>\n",
       "      <td>negative</td>\n",
       "      <td>i hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>440e8fc447</td>\n",
       "      <td>Hockey was so fukinï¿½ good  **** you hole! xD</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hockey was so fukinï¿½ good **** you hole! xd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3065</th>\n",
       "      <td>7ff53f335c</td>\n",
       "      <td>u can see I just twitted them if its the real xmsirius ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>u can see i just twitted them if its the real xmsirius ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>32e183bbad</td>\n",
       "      <td>s/s aus fshionwk- zimmermann,illionare,dhini + gail sorr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>loved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>aeb56209ce</td>\n",
       "      <td>I`m glad someone slept with me last night. My doggy  I w...</td>\n",
       "      <td>positive</td>\n",
       "      <td>glad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>16ee71e6df</td>\n",
       "      <td>Sadly I think I know exactly were you put it--in the ex...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>5f1811bb3d</td>\n",
       "      <td>Good morning twitterland. Happy Monday</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>052fbbb0a9</td>\n",
       "      <td>Sat alone whilst jay plays on his phone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sat alone whilst jay plays on his phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>baeb21b26f</td>\n",
       "      <td>**** these nig*s is all on me, but they won`t get this.....</td>\n",
       "      <td>negative</td>\n",
       "      <td>****</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>eba3cee9be</td>\n",
       "      <td>_Brown Thank you so much Natalie, hope u are well</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank you so much natalie, hope u are well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>c4453ff2b8</td>\n",
       "      <td>Thanks for sending the link.</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>762a44afb6</td>\n",
       "      <td>Mrs.Bates left</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mrs.bates left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3288</th>\n",
       "      <td>d53ddd0172</td>\n",
       "      <td>i`m not feeling twitter at the moment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i`m not feeling twitter at the moment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "87    cde0016d6d  http://twitpic.com/4wp8s - My ear hurts, and THIS is my ...   \n",
       "2701  acf7eddf2b   ****. You could have just called or told me in person. ...   \n",
       "589   7315faa6ce                        oh mannn i`m gonna be there tomorroww   \n",
       "1822  5a8f396cb2  so who is in for bring at HK lounge tomorrow ? $12 all y...   \n",
       "1644  fc6128f4e5  ok i just spent like ï¿½50 on soundtracks, a galaxy clas...   \n",
       "1839  c4393e4cba   HAPPY MOTHERS DAY!  Tell ur mom that`s she an awesome m...   \n",
       "1596  6612e42b28                                      Kyle is Cody`s wee bro!   \n",
       "2687  4e05652b8c                                       Time to play the drums   \n",
       "1952  63491134ab   i made a vid for you proving my skiLLs that you denied ...   \n",
       "2539  ba7df25a0e                 Chrystina Grace Timberlake has a ring to it!   \n",
       "2297  822cf03d0d   Internet 1 drawing 0 ....**** I`m backing up blackberry...   \n",
       "1294  d65a8658b0    I just got through doing my work now I can take the wh...   \n",
       "242   53969f2468                    Hey! That`s my school! I hate that place.   \n",
       "1730  440e8fc447               Hockey was so fukinï¿½ good  **** you hole! xD   \n",
       "3065  7ff53f335c   u can see I just twitted them if its the real xmsirius ...   \n",
       "2276  32e183bbad  s/s aus fshionwk- zimmermann,illionare,dhini + gail sorr...   \n",
       "2574  aeb56209ce  I`m glad someone slept with me last night. My doggy  I w...   \n",
       "188   16ee71e6df   Sadly I think I know exactly were you put it--in the ex...   \n",
       "881   5f1811bb3d                       Good morning twitterland. Happy Monday   \n",
       "2609  052fbbb0a9                      Sat alone whilst jay plays on his phone   \n",
       "1650  baeb21b26f  **** these nig*s is all on me, but they won`t get this.....   \n",
       "1654  eba3cee9be            _Brown Thank you so much Natalie, hope u are well   \n",
       "1545  c4453ff2b8                                 Thanks for sending the link.   \n",
       "1505  762a44afb6                                               Mrs.Bates left   \n",
       "3288  d53ddd0172                        i`m not feeling twitter at the moment   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "87    negative                                                       hurts,  \n",
       "2701  negative                                                        ****.  \n",
       "589    neutral                        oh mannn i`m gonna be there tomorroww  \n",
       "1822  positive                                                         nice  \n",
       "1644   neutral   ok i just spent like ï¿½50 on soundtracks, a galaxy cla...  \n",
       "1839  positive                                                        happy  \n",
       "1596   neutral                                      kyle is cody`s wee bro!  \n",
       "2687   neutral                                       time to play the drums  \n",
       "1952  negative   i made a vid for you proving my skiLLs that you denied ...  \n",
       "2539   neutral                 chrystina grace timberlake has a ring to it!  \n",
       "2297  negative   internet 1 drawing 0 ....**** i`m backing up blackberry...  \n",
       "1294  positive                                                    its great  \n",
       "242   negative                                                       i hate  \n",
       "1730   neutral                hockey was so fukinï¿½ good **** you hole! xd  \n",
       "3065   neutral   u can see i just twitted them if its the real xmsirius ...  \n",
       "2276  positive                                                        loved  \n",
       "2574  positive                                                         glad  \n",
       "188   negative                                                        sadly  \n",
       "881   positive                                                 happy monday  \n",
       "2609   neutral                      sat alone whilst jay plays on his phone  \n",
       "1650  negative                                                         ****  \n",
       "1654  positive                   thank you so much natalie, hope u are well  \n",
       "1545  positive                                                       thanks  \n",
       "1505   neutral                                               mrs.bates left  \n",
       "3288   neutral                        i`m not feeling twitter at the moment  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(25)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
